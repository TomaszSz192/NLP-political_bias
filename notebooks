import nltk
import pandas as pd
from google.colab import drive
drive.mount('/content/drive',force_remount=True)

df = pd.read_csv('/content/drive/MyDrive/NLP/Political_Bias.csv', usecols=['Title', 'Text','Bias'])
'df = pd.read_csv("Political_Bias.csv")'
df.columns = df.columns.str.lower()
print(df.head())
print(df.columns)


from spacy.lang.en.stop_words import STOP_WORDS

stopwords = list(STOP_WORDS)

print("\n****************************************\n")
print(f"Stop words w tej liście jest {len(stopwords)}, a wyglądają następująco:\n\n",stopwords)
import re
from string import punctuation

print("\n****************************************\n")
print("Interpunkcja:",punctuation)

def clean_text(text):
    temp = text.lower() # Cały tekst na małe litery, aby nie było różnic
    temp = re.sub('\d', '', temp) # Wszystkie cyfry '\d' zamieni na nic ''
    temp = re.sub('<[^>]*>', '', temp)
    emojis = re.findall('(?::|;|=)(?:-)?(?:\)|\(|D|P)', temp)
    temp = re.sub('[\W]+', ' ', temp) + ' '.join(emojis).replace('-', '')
    temp = re.sub('[{}]'.format(punctuation), '', temp)
    temp = temp.strip() # Usunięcie białych znaków na początku i końcu
    return temp # Zwracamy oczyszczony tekst
    # Stemming:

from nltk.stem.porter import PorterStemmer
porter = PorterStemmer() # obiekt, który będzie wykorzystany do stemmingu w funkcji text_tokenizer

# Deklaracja funkcji, która będzie przekazywana do vectorizatora -- w celu oczyszczenia tekstu
def text_tokenizer(text):
    text = clean_text(text)
    words_after_stem = [porter.stem(word) for word in text.split()] # split dzieli na pojedyncze słowa, a potem ten stemmer je przetwarza
    return  [word for word in words_after_stem if word not in stopwords and len(word) > 2] # do tego usuwamy te, które są za krótkie lub są w stopwords


from sklearn.model_selection import train_test_split

# Deklaracja i definicja funkcji, w której będziemy przetwarzać i dzielić dane
def split_and_vectorize_text(vectorizer, X, y, test_size = 0.3):
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size)

  X_train_transform = vectorizer.fit_transform(X_train)
  X_test_transform = vectorizer.transform(X_test)
  print('Wielkość danych po przetworzeniu:', X_train_transform.shape) # To dla trenujących, ale chodzi o to, że widzimy ile powstało kolumn (kolumna, to słowo). W testowych kolumn będzie tyle samo
  return X_train_transform, X_test_transform, y_train, y_test

#3. Oczyszczone dane -- mamy już zdefiniowane wszystkie metody do czyszczenia i transformacji, a więc przechodzimy do ich zastosowania
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

method = "TF-IDF"

test_size = 0.33

# Tworzenie wektoryzera, według  metody TF-IDF
# Tokenizer zawsze jest zgodny z tym, co mamy w funkcji text_tokenizer,
#natomiast TfidfVectorizer, to TF-IDF
vectorizer = TfidfVectorizer(tokenizer=text_tokenizer)
if method == "Count":
    vectorizer = CountVectorizer(tokenizer=text_tokenizer, binary=False)
elif method == "Binary":
    vectorizer = CountVectorizer(tokenizer=text_tokenizer, binary=True)

X_train, X_test, y_train, y_test = split_and_vectorize_text(vectorizer, df['title'], df['bias'], test_size)
# Tutaj już mamy atrybuty warunkowe do trenowania, testowania, atrybut decyzyjny dla zbioru trenującego i testowego.

print("Dane po pełnym przetworzeniu i podziale -- przykładowo, część trenująca:")
print(X_train)
print(y_train)
print("\n****************************************\n")

# 4 i 5. Eksploracja danych i uzyskane wyniki

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

classifier = DecisionTreeClassifier(max_depth=10)

classifier.fit(X_train, y_train) # Uczymy drzewo na danych treningowych

y_pred = classifier.predict(X_test)

print("Wyniki klasyfikacji na zbiotrze testowym:")
print(classification_report(y_test, y_pred)) # Sprawdzamy, czy predykcja jest zgodna z rzeczywistymi wartościami dacyzji
print("\n****************************************\n")

cm = confusion_matrix(y_test, y_pred)
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = ['lean right','lean left','right','left','center'])
fig, ax = plt.subplots(figsize=(20,10))

print(cm)
cm_display.plot(ax=ax)
plt.show()

# 6. Wizualizacja

import matplotlib.pyplot as plt
from prettytable import PrettyTable

# Definicja i deklaracja funkcji

def aggregate_all_columns(np_matrix, aggregate_type='mean'):
  if aggregate_type == 'sum':
    return np_matrix.sum(axis=0).tolist()[0]
  elif aggregate_type == 'mean':
    return np_matrix.mean(axis=0).tolist()[0]

  raise AttributeError('Undefined aggregare type')

def sort_dict_by_values(x, desc=True):
    return dict(sorted(x.items(), key=lambda item: item[1], reverse=desc))

def create_bow_from_means_and_feature_names(means, feature_names):
    return {token: means[i] for i, token in enumerate(feature_names)}

def calculate_features_weight(X, is_tfidf_mode=True, is_binary_mode=False, aggregate_weigth_type='mean'):
    X = X.dropna()
    if X.empty:
        print(df['bias'].unique())  # Jakie wartości są w kolumnie 'bias'?
        print(df[df['bias'] == 1])  # Czy są jakieś wiersze z bias == 1?
        raise ValueError("Przekazany zbiór dokumentów jest pusty.")

    sample_tokens = text_tokenizer(X.iloc[0])
    if not sample_tokens:
        raise ValueError("Funkcja text_tokenizer nie zwraca żadnych tokenów. Sprawdź jej działanie.")

    if is_tfidf_mode:
        vectorizer = TfidfVectorizer(tokenizer=text_tokenizer)
    else:
        vectorizer = CountVectorizer(tokenizer=text_tokenizer, binary=is_binary_mode)

    X_transform = vectorizer.fit_transform(X)
    means_for_features = aggregate_all_columns(X_transform, aggregate_weigth_type)
    bow = create_bow_from_means_and_feature_names(means_for_features, vectorizer.get_feature_names_out())
    return sort_dict_by_values(bow)

def visualize_bow_terms(bow, title, second_column_title='Weigth', top_n=15, pdf_name="fig.pdf"):
    plt.barh(list(bow.keys())[:top_n], list(bow.values())[:top_n])
    plt.title(title)
    plt.gca().invert_yaxis()
    plt.show()
    plt.savefig(pdf_name)
    plt.clf()

    table = PrettyTable()

    table.title = title
    table.add_column('Term', list(bow.keys())[:top_n])
    table.add_column(second_column_title, list(bow.values())[:top_n])
    print(table)

top_n=15

# visualize_bow_terms, to "nasza" funkcja
visualize_bow_terms(
             calculate_features_weight(df[df['bias'] == 'right']['text']),
    'Crucial tokens based on TF-IDF weight: RIGHT Bias (Top 15)',
    top_n=top_n,
    pdf_name="fig_TF-IDF_right.pdf"
)

visualize_bow_terms(
    calculate_features_weight(df[df['bias'] == 'left']['text']),
    'Crucial tokens based on TF-IDF weight: LEFT Bias (Top 15)',
    top_n=top_n,
    pdf_name="fig_TF-IDF_left.pdf"
)

visualize_bow_terms(
    calculate_features_weight(df[df['bias'] == 'lean right']['text']),
    'Crucial tokens based on TF-IDF weight: LEAN RIGHT Bias (Top 15)',
    top_n=top_n,
    pdf_name="fig_TF-IDF_lean right.pdf"
)

visualize_bow_terms(
    calculate_features_weight(df[df['bias'] == 'lean left']['text']),
    'Crucial tokens based on TF-IDF weight: LEAN LEFT Bias (Top 15)',
    top_n=top_n,
    pdf_name="fig_TF-IDF_lean left.pdf"
)

visualize_bow_terms(
    calculate_features_weight(df[df['bias']== 'center']['text']),
    'Crucial tokens based on TF-IDF weight: CENTER Bias (Top 15)',
    top_n=top_n,
    pdf_name="fig_TF-IDF_center.pdf"
)

print("Koniec zadania")
